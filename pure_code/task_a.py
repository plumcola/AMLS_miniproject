# -*- coding: utf-8 -*-
"""task_A.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GlIhy-l_B8BruM0egQzwwfaY0sGKd-dy
"""

#Import the libraries weâ€™ll need
import os
import shutil 
import pandas as pd
import keras
import numpy as np
from os import listdir
import tensorflow as tf
import cv2
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D,BatchNormalization,ZeroPadding2D
from tensorflow.keras.utils import to_categorical
from keras.preprocessing import image
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.layers.normalization import layer_normalization
from keras.layers import Activation
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score
from tensorflow.keras.callbacks import EarlyStopping
from keras.utils.np_utils import normalize
from sklearn import svm

#The function for processing dataset into tumor and non_tumor
def split_data_into_no_yes_dirs(dir):
  os.chdir(dir)
  train_csv = pd.read_csv("label.csv")
  #print(train_csv.head())
  directory = os.fsencode('image')
  for file in os.listdir(directory):
      filename = os.fsdecode(file)
      index = train_csv.index[train_csv['file_name'] == filename]
      if train_csv.loc[index.values[0],'label']=="no_tumor":
          shutil.copy('image/'+filename, '/content/drive/MyDrive/AMLS/dataset/no')
      else:
          shutil.copy('image/'+filename, '/content/drive/MyDrive/AMLS/dataset/yes')

#Invoke the previous function

# mount google drive to load the dataset uploaded on it 
from google.colab import drive
drive.mount('/content/drive')

split_data_into_no_yes_dirs("/content/drive/MyDrive/AMLS/dataset")

#The function for loading training data
def load_train_data(dir):
    X = []
    y = []
    os.chdir(dir)
    dirs = ['yes', "no"]
    for directory in dirs:
        for filename in listdir(directory):
            image = cv2.imread(directory+'/'+filename)
            image = cv2.resize(image, dsize=(240,240))
            X.append(image)
            if directory == 'yes':
                y.append([1])
            elif directory == 'no':
                y.append([0])
            else:
              print("Wrong! ")
    X = np.array(X)
    X = normalize(X, axis=1)
    y = np.array(y)
    return X, y

#Invoke the previous function
data_dir ="/content/drive/MyDrive/AMLS/dataset"
X, y = load_train_data(data_dir)

print(f'Number of examples is: {len(X)}')
print(f'X shape is: {X.shape}')
print(f'y shape is: {y.shape}')

#Divide the dataset
X_train , X_test, y_train, y_test = train_test_split(X , y , test_size = 0.1, random_state = 42)
X_train , X_val, y_train, y_val = train_test_split(X_train , y_train , test_size = 0.1, random_state = 42)

#The function for building the CNN model for task A
def build_model():
  tf.keras.backend.clear_session()
  model= Sequential()
  model.add(Conv2D(128, kernel_size =(6,6),padding='same', input_shape=(240,240,3)))
  model.add(Activation('relu'))
  model.add(BatchNormalization())
  model.add(MaxPooling2D((2,2)))
  model.add(Dropout(0.2))
  model.add(Conv2D(96, kernel_size =(6,6), padding='same', strides=[2,2]))
  model.add(Activation('relu'))
  model.add(BatchNormalization())
  model.add(MaxPooling2D((2,2)))
  model.add(Dropout(0.2))


  model.add(Flatten())
  model.add(Dense(units = 1, activation = 'sigmoid'))
  
  return model

#The function for getting accuracy metrics
def get_accuracy_metrics(X_test, y_test):
  print("Test Acc: ",model.evaluate(X_test, y_test, verbose=0)[1])

  pred = model.predict(X_test)
  print("Confusion Matrix : \n ",confusion_matrix(np.round(pred), y_test))
  print("\nprecision score:  ",precision_score(np.round(pred), y_test))
  print("\nrecall score: ",recall_score(np.round(pred), y_test))
  print("\nF1 Score: ",f1_score(np.round(pred), y_test))

#The function for ploting learning curves
def plot_learning_curves(history):  
  train_loss = history['loss']
  val_loss = history['val_loss']
  train_acc = history['accuracy']
  val_acc = history['val_accuracy']

  plt.figure()
  plt.plot(train_loss, label='Training Loss')
  plt.plot(val_loss, label='Validation Loss')
  plt.title('Loss')
  plt.legend()
  plt.show()

  plt.figure()
  plt.plot(train_acc, label='Training Accuracy')
  plt.plot(val_acc, label='Validation Accuracy')
  plt.title('Accuracy')
  plt.legend()
  plt.show()

#Train the CNN model for task A
model = build_model()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(x=X_train, y=y_train, batch_size=32, epochs=20, validation_data=(X_val, y_val))

from tensorflow.keras.utils import plot_model

plot_model(model,to_file='model.png',show_shapes=True, show_layer_names=False)

!pip install visualkeras

import visualkeras
from PIL import ImageFont
#model = build_model()
visualkeras.layered_view(model, legend=True,draw_volume=False, scale_xy=2,scale_z=0.000001)

get_accuracy_metrics(X_test, y_test)

plot_learning_curves(history.history)

#The function for loading test data in order to provide scores
def load_test_data(dir):
  y_test = []
  X_test = [] 
  os.chdir(dir)

  y_test_csv = pd.read_csv('label.csv')
  image_name=y_test_csv['file_name'].to_list()
  label_name=y_test_csv['label'].to_list()
  for i in range(len(label_name)):  
    if label_name[i] == 'no_tumor':
        y_test.append(0)
    else:
        y_test.append(1)
  y_test=np.array(y_test)

  directory = "image"
  for i in range(len(image_name)):
      image = (directory+'/'+image_name[i])
      image = cv2.imread(image)
      image = cv2.resize(image,(240,240))
      X_test.append(image)
  X_test = np.array(X_test)
  X_test=normalize(X_test, axis=1)
  return X_test,y_test

dir = "/content/drive/MyDrive/AMLS/test"
X_test_data,y_test_data = load_test_data(dir)

get_accuracy_metrics(X_test_data, y_test_data)

#SVM for task A

# creating an SVM Classifier
svc_classifier = svm.SVC(kernel='linear') # Linear Kernel
# training the model using the training sets
svc_classifier.fit(X_train.reshape(X_train.shape[0],240*240*3), y_train)

# predicting the response for test dataset
y_pred = svc_classifier.predict(X_test.reshape(X_test.shape[0],240*240*3))

# reporting model accuracy
print("Accuracy:", round(accuracy_score(y_test.ravel(), y_pred)*100),'%')
print("Confusion Matrix : \n ",confusion_matrix(y_pred, y_test))
print("\nprecision score:  ",precision_score(y_pred, y_test))
print("\nrecall score: ",recall_score(y_pred, y_test))
print("\nF1 Score: ",f1_score(y_pred, y_test))
